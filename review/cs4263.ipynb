{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NLP / Sentiment Analysis Introduction \n",
    "##### What is NLP / Sentiment Analsis?\n",
    "---\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of AI concerned with enabling machines \n",
    "with the ability to understand, analyze, and generate natural human language. Sentiment Analysis is \n",
    "the subsect of NLP concerned with classifying the polarity or emotion of a block of input text. \n",
    "Applications include Customer Feedback Analysis, Brand Reputation, Competitor Analysis, Marketing \n",
    "effectivness, and so much more! \n",
    "\n",
    "Types of Sentiment Analysis include :\n",
    "1. Emotion Detection \n",
    "2. Aspect-Based Analysis\n",
    "3. Multi-lingual Analysis\n",
    "4. Fine-Grained \n",
    "5. Rule/Sentiment Based "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55a9d6763db99976"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial Approaches at Sentiment Analysis\n",
    "##### Basic Lexicon Sentiment Analysis\n",
    "---\n",
    "In the early approaches, Sentiment Analysis models relied on statically defined *Lexicons* (essentially a list of keywords) that identifies words of interest. Each keyword in the lexicon is mapped to a *polarity* (positive (1) / negative (-1) ). From there, we can easily scan a block of text, keeping track of how many positive and negative words we encounter.  \n",
    "\n",
    "Straight-Forward Implementation\n",
    "1. Define static lexicon (or vocabulary) of positive and negative words\n",
    "2. Iterate through input stream\n",
    "3. Add to score when positive, penalize from score when negative\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a2bdaa7606804a7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# --------- Static Lexicon---------\n",
    "#   Positive sentiment = +1 \n",
    "#   Negative sentiment = -1\n",
    "lexicon = {\"good\": 1, \"great\": 1, \"excellent\": 1, \"wonderful\": 1, \"amazing\": 1, \"fantastic\": 1,\n",
    "           \"bad\": -1, \"terrible\": -1, \"awful\": -1, \"horrible\": -1, \"stupid\": -1}\n",
    "\n",
    "def simple_sentiment(text: str) -> str:\n",
    "  '''\n",
    "  Initial Sentiment Analysis Approach, count positive and negative words\n",
    "  :param text: Block of text to be analyzed\n",
    "  :return: string result \"POS\"/\"NEG\"/\"NEUTRAL\"\n",
    "  '''\n",
    "  tokens = text.split()\n",
    "  score = 0\n",
    "  for token in tokens:\n",
    "    score += lexicon.get(token.lower(), 0)\n",
    "  return \"POS\" if score > 0 else \"NEG\" if score < 0 else \"neutral\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T00:03:13.901160300Z",
     "start_time": "2025-03-24T00:03:13.896080300Z"
    }
   },
   "id": "a4a8d5d773a051a8",
   "execution_count": 183
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: POS\t=>\t<POS>\n",
      "Expected: POS\t=>\t<POS>\n",
      "\n",
      "Expected: NEG\t=>\t<NEG>\n",
      "Expected: NEG\t=>\t<POS>\n",
      "\n",
      "\n",
      "Expected: NEG\t=>\t<POS>\n",
      "Expected: NEG\t=>\t<neutral>"
     ]
    }
   ],
   "source": [
    "#positive test\n",
    "print(\"Expected: POS\\t=>\\t<\" + simple_sentiment(\"This is a good movie!\") + \">\")\n",
    "print(\"Expected: POS\\t=>\\t<\" + simple_sentiment(\"This is a very good movie!\") + \">\", end='\\n\\n')\n",
    "#negative test\n",
    "print(\"Expected: NEG\\t=>\\t<\" + simple_sentiment(\"This is a bad movie!\") + \">\")\n",
    "print(\"Expected: NEG\\t=>\\t<\" + simple_sentiment(\"That guy is not a good person\") + \">\", end='\\n\\n')\n",
    "#confusing it on purpose\n",
    "print(\"\\nExpected: NEG\\t=>\\t<\" + simple_sentiment(\"I don't feel very good today.\") + \">\", end='')\n",
    "print(\"\\nExpected: NEG\\t=>\\t<\" + simple_sentiment(\"I do not like The Amazing Spiderman. The visuals were good, but overall the storyline was awful and predictable.\") + \">\", end='')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T02:38:59.330388900Z",
     "start_time": "2025-03-24T02:38:59.323599300Z"
    }
   },
   "id": "ca47d4412682e975",
   "execution_count": 211
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Strengths and Limitations \n",
    "##### Lexicon Based Methods \n",
    "- - -\n",
    "Even though this is simplistic implementation of a lexicon-based model, the pitfalls are clearly visible. That is the tradeoff we accept, as lexicon-based models are simple to implement and interpret. We can clearly determine *WHY* a given decision was made. Additionally, no training overhead is required. No need for gathering up a labeled dataset that fits your use-case. \n",
    "\n",
    "That being said, even with a well-designed lexicon model, the pitfalls are signifcant. There is a reason why better models were quickly produced after all.  Just to name a few of the problems here: \n",
    "1. Fails to consider context\n",
    "2. Does not handle negation (That guy is NOT a good person) \n",
    "3. Does not handle intensity or sarcasm\n",
    "4. Predefined lexicons do not adapt with language or respond well to unknown words\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c169d6e3912593cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Improving Initial Approach \n",
    "##### Rule Handling and Lexicon Improvements \n",
    "---\n",
    "To improve the limited lexicon-based approach, we must handle the following:\n",
    "1. Negation handling\n",
    "2. Intensity handling\n",
    "3. Dynmaic / Domain-specific lexicon\n",
    "4. Tokenization\n",
    "5. Multi-Word Phrase Recognition\n",
    "\n",
    "People who are much better at programming than I came up with something called VADER. VADER stands for Valence Aware Dictionary and sEntiment Reasonser, and it is a lexicon + rule based model. It includes a lexicon which includes the typical sentiment words, but also accounts for slang (such as \"meh\"). Each word is mapped to a valence score in addition to the set of defined rules to handle context. \n",
    "\n",
    "\n",
    "These improvements to the simplistic lexicon-based model allow for VADER to be extremely effective for classifying blocks of text found in social media postings and product reviews (think Twitter, Instragram, etc). That being said, it is still a Lexicon-Rule Based approach, still reaching its limitations when confronted with sarcasm or more complex tasks. \n",
    "\n",
    "Implementing all that from scratch can get rather expansive, so here is a pretty simple implementation to serve as a basic view into how one might implement that"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1a66663922e30f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#1. Negation Handling\n",
    "#2. Intensity Handling\n",
    "\n",
    "#------- Statically Defined Vocabularies -----\n",
    "\n",
    "lexicon_plus = {\"good\": 1, \"great\": 2, \"excellent\": 3, \"wonderful\": 2, \"amazing\": 2, \"fantastic\": 3,\n",
    "                \"bad\": -1, \"terrible\": -2, \"awful\": -2, \"horrible\": -3, \"stupid\": -2}\n",
    "negation_words  = [\"not\", \"don't\", \"dont\", \"never\", \"no\"]\n",
    "amplifier_words = {\"extremely\": 2.0, \"very\": 1.5, \"so\": 1.2, \"really\": 1.3}\n",
    "\n",
    "def simple_sentiment_plus(text: str) -> str:\n",
    "  '''\n",
    "  slight improvement on simple_sentiment\n",
    "  Adds rules for handling negation and intensity\n",
    "  :param text: Block of text to be analyzed\n",
    "  :return: string result \"POS\"/\"NEG\"/\"NEUTRAL\"\n",
    "  '''\n",
    "  tokens = text.split()\n",
    "  negate_window = 0\n",
    "  multiplier    = 1.0\n",
    "  score = 0\n",
    "  \n",
    "  #define some list of negators and amplifiers\n",
    "\n",
    "  for token in tokens:\n",
    "    word = token.lower().strip(\".,?!()_\\\"\\'!@#$%^&*+=:;\")\n",
    "    value = 0\n",
    "    if word in negation_words:\n",
    "      negate_window = 3\n",
    "      continue\n",
    "\n",
    "    if word in amplifier_words:\n",
    "      multiplier *= amplifier_words[word]\n",
    "      continue\n",
    "\n",
    "    if word in lexicon_plus:\n",
    "      value = lexicon_plus[word]\n",
    "\n",
    "      if negate_window > 0:\n",
    "        value = -value\n",
    "        negate_window -= 1\n",
    "\n",
    "    score += value * multiplier\n",
    "    multiplier = 1.0\n",
    "\n",
    "  return \"POS\" if score > 0 else \"NEG\" if score < 0 else \"neutral\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T04:31:08.659411400Z",
     "start_time": "2025-03-24T04:31:08.653961600Z"
    }
   },
   "id": "8956017a269f93ad",
   "execution_count": 234
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: POS\t=>\t<POS>\n",
      "Expected: POS\t=>\t<POS>\n",
      "\n",
      "Expected: NEG\t=>\t<NEG>\n",
      "Expected: NEG\t=>\t<NEG>\n",
      "\n",
      "Expected: NEG\t=>\t<NEG>\n",
      "Expected: NEG\t=>\t<NEG>"
     ]
    }
   ],
   "source": [
    "#positive test\n",
    "print(\"Expected: POS\\t=>\\t<\" + simple_sentiment_plus(\"This is a good movie!\") + \">\")\n",
    "print(\"Expected: POS\\t=>\\t<\" + simple_sentiment_plus(\"This is a very good movie!\") + \">\", end='\\n\\n')\n",
    "#negative test\n",
    "print(\"Expected: NEG\\t=>\\t<\" + simple_sentiment_plus(\"This is a bad movie!\") + \">\")\n",
    "print(\"Expected: NEG\\t=>\\t<\" + simple_sentiment_plus(\"That guy is not a good person\") + \">\", end='\\n\\n')\n",
    "#confusing it on purpose\n",
    "print(\"Expected: NEG\\t=>\\t<\" + simple_sentiment_plus(\"I don't feel very good today.\") + \">\", end='\\n')\n",
    "print(\"Expected: NEG\\t=>\\t<\" + simple_sentiment_plus(\"I do not like The Amazing Spiderman. The visuals were good, but overall the storyline was awful and predictable. not good at all\") + \">\", end='')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T04:31:08.888256900Z",
     "start_time": "2025-03-24T04:31:08.881438700Z"
    }
   },
   "id": "55890c4d46391092",
   "execution_count": 235
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this product! It works great and is very affordable.\n",
      "{'neg': 0.0, 'neu': 0.482, 'pos': 0.518, 'compound': 0.8622}\n",
      "\n",
      "This product is okay. It gets the job done, but could be better.\n",
      "{'neg': 0.0, 'neu': 0.675, 'pos': 0.325, 'compound': 0.6486}\n",
      "\n",
      "I hate this product. It doesn't work at all and is a waste of money.\n",
      "{'neg': 0.371, 'neu': 0.629, 'pos': 0.0, 'compound': -0.7579}\n"
     ]
    }
   ],
   "source": [
    "#nltk VADER example \n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "samples = [\n",
    "    \"I love this product! It works great and is very affordable.\",\n",
    "    \"This product is okay. It gets the job done, but could be better.\",\n",
    "    \"I hate this product. It doesn't work at all and is a waste of money.\"\n",
    "]\n",
    "\n",
    "for text in samples:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    print(text)\n",
    "    print(scores, end='\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T01:51:30.250909500Z",
     "start_time": "2025-03-25T01:51:30.243974700Z"
    }
   },
   "id": "ce0f4fb51d7cab4c",
   "execution_count": 256
  },
  {
   "cell_type": "markdown",
   "source": [
    "While combining Lexicon and Rule based methods improves performance significantly, maintaining and scaling such systems is cumbersome and just not really worth it. They can easily break, and will never properly encapsulate all the ways people convey sentiment. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58c86b3877b175b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next Generation Techniques \n",
    "##### Overcoming Lexicon + Rule Based Limitations\n",
    "- - -\n",
    "Eventually some smart person finally got tired of having to hand-write rules  and update their lexicons constantly and asked themselves - *\"Why do I hate my life, and how can I get the computer to do this for me?\"* - and all of a sudden shit got real\n",
    "\n",
    "In the following section, I aim to cover some of the approaches to \n",
    "1. Dynamic Lexicon Creation,\n",
    "2. Tokenization,\n",
    "3. Mutli-Phrase detection,\n",
    "4. N-Grams, \n",
    "5. and possibly more..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fdcf427b9b02b6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Dynamic Lexicon Creation\n",
    "##### How statistics took us a step further\n",
    "---\n",
    "The idea here is simple. How can we overcome the need for manually created lexicons by learning sentiment from data? \n",
    "\n",
    "The answer is, well, there were many techniques developed. \n",
    "1. Corpus-Based Lexicon Expansion\n",
    "2. Semantic Orientation\n",
    "3. Sentiment Classification via Clustering\n",
    "4. many more...\n",
    "For simplicity and time sake, I will focus on number 1 *Corpus-Based Lexicon Expasion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b5326c0a19dc3d0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mwordnet\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/wordnet\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Trevor/nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\PycharmProjects\\\\Practice\\\\.venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\PycharmProjects\\\\Practice\\\\.venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\PycharmProjects\\\\Practice\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mLookupError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Practice\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001B[39m, in \u001B[36mLazyCorpusLoader.__load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     83\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m84\u001B[39m     root = \u001B[43mnltk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mzip_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     85\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Practice\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001B[39m, in \u001B[36mfind\u001B[39m\u001B[34m(resource_name, paths)\u001B[39m\n\u001B[32m    578\u001B[39m resource_not_found = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m579\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[31mLookupError\u001B[39m: \n**********************************************************************\n  Resource \u001B[93mwordnet\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/wordnet.zip/wordnet/\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Trevor/nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\PycharmProjects\\\\Practice\\\\.venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\PycharmProjects\\\\Practice\\\\.venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\PycharmProjects\\\\Practice\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mLookupError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[263]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     10\u001B[39m           synonyms.add(lem)\n\u001B[32m     11\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m synonyms\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mget_synoynms\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mterrible\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[263]\u001B[39m\u001B[32m, line 7\u001B[39m, in \u001B[36mget_synoynms\u001B[39m\u001B[34m(word)\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_synoynms\u001B[39m(word: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m      6\u001B[39m   synonyms = \u001B[38;5;28mset\u001B[39m()\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m   \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[43mwordnet\u001B[49m\u001B[43m.\u001B[49m\u001B[43msynsets\u001B[49m(word):\n\u001B[32m      8\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m lem \u001B[38;5;129;01min\u001B[39;00m w.lemmas(): \n\u001B[32m      9\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m lem \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m synonyms: \n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Practice\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001B[39m, in \u001B[36mLazyCorpusLoader.__getattr__\u001B[39m\u001B[34m(self, attr)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m attr == \u001B[33m\"\u001B[39m\u001B[33m__bases__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    118\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mLazyCorpusLoader object has no attribute \u001B[39m\u001B[33m'\u001B[39m\u001B[33m__bases__\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m \u001B[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001B[39;00m\n\u001B[32m    122\u001B[39m \u001B[38;5;66;03m# __class__ to something new:\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, attr)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Practice\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001B[39m, in \u001B[36mLazyCorpusLoader.__load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     84\u001B[39m             root = nltk.data.find(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.subdir\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzip_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     85\u001B[39m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m86\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m     88\u001B[39m \u001B[38;5;66;03m# Load the corpus.\u001B[39;00m\n\u001B[32m     89\u001B[39m corpus = \u001B[38;5;28mself\u001B[39m.__reader_cls(root, *\u001B[38;5;28mself\u001B[39m.__args, **\u001B[38;5;28mself\u001B[39m.__kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Practice\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001B[39m, in \u001B[36mLazyCorpusLoader.__load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     79\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     80\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m         root = \u001B[43mnltk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     82\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     83\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Practice\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001B[39m, in \u001B[36mfind\u001B[39m\u001B[34m(resource_name, paths)\u001B[39m\n\u001B[32m    577\u001B[39m sep = \u001B[33m\"\u001B[39m\u001B[33m*\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m70\u001B[39m\n\u001B[32m    578\u001B[39m resource_not_found = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m579\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[31mLookupError\u001B[39m: \n**********************************************************************\n  Resource \u001B[93mwordnet\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/wordnet\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Trevor/nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\PycharmProjects\\\\Practice\\\\.venv\\\\nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\PycharmProjects\\\\Practice\\\\.venv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\PycharmProjects\\\\Practice\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Trevor\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Corpus Based Lexicon Expansion\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_synoynms(word: str) -> list[str]:\n",
    "  synonyms = set()\n",
    "  for w in wordnet.synsets(word):\n",
    "    for lem in w.lemmas(): \n",
    "        if lem not in synonyms: \n",
    "          synonyms.add(lem)\n",
    "  return synonyms\n",
    "\n",
    "print(f'{get_synoynms(\"terrible\")}')\n",
    "   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T07:56:21.480288600Z",
     "start_time": "2025-03-25T07:56:21.238161800Z"
    }
   },
   "id": "71838b4fede75b85",
   "execution_count": 263
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization \n",
    "##### What is it and Why am I talking about it?\n",
    "---\n",
    "\n",
    "Tokenization is defined as breaking text into sequences of *Tokens*. There are many different ways in which we can *Tokenize* our input text\n",
    "1. Character Tokenization\n",
    "    - Split text by each character\n",
    "2. Word Tokenization\n",
    "    - Split text by each word \n",
    "3. Subword Tokenization\n",
    "    - Breaks words down into smaller units\n",
    "\n",
    "Why bother with Tokenization? By breaking the text block into smaller tokens, we enable the computer with the ability to identify meaningful features and patterns within language. We can standardize our input (i.e. stripping whitespaces, stop words, ...), we can generate vocabularies, and later on convert our *tokens* into numerical representations. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6103f7d8c481c026"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#3. Tokenization\n",
    "import re\n",
    "\n",
    "def character_tokenization(text: str) -> list:\n",
    "  '''\n",
    "  tokenizez text at the character level\n",
    "  :param text: block of text to be tokenzied\n",
    "  :return: list of tokens \n",
    "  '''\n",
    "  tokens = []\n",
    "  for char in text:\n",
    "    tokens.append(char)\n",
    "  return tokens\n",
    "\n",
    "#splits input by words (spaces)\n",
    "def word_tokenization(text: str) -> list:\n",
    "  '''\n",
    "  tokenize text at the word level (split by spaces)\n",
    "  :param text: block of text to be tokenzied\n",
    "  :return: list of tokens\n",
    "  ''' \n",
    "  #text.split() may be sufficient in some cases\n",
    "  pattern = r\"[A-Za-z]+(?:'[A-Za-z]+)?|[.,!?;]\"\n",
    "  return re.findall(pattern, text.lower())\n",
    "\n",
    "#stem extraction\n",
    "''' i would implement myself, but I wanted to focus on the bigger parts\n",
    "i started it and gave up its so if else if else if else if else if else if else if else if else if else if else if else if else if'''\n",
    "from nltk import PorterStemmer\n",
    "def stem_tokenization(text: str):\n",
    "  stemmer = PorterStemmer()\n",
    "  words = word_tokenization(text)\n",
    "  return  [stemmer.stem(word) for word in words]\n",
    "\n",
    "#lemmatization\n",
    "''' again i would implement myself, but i wanna focus on getting somewhere in the project first\n",
    "i can always come back to this spot and implement from scratch should i choose or have the time for it'''\n",
    "from nltk import WordNetLemmatizer\n",
    "def lemmatize_tokenization(text: str):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words = word_tokenization(text)\n",
    "  print(words)\n",
    "  return [lemmatizer.lemmatize(word) for word in words]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T09:31:23.240971100Z",
     "start_time": "2025-03-24T09:31:23.234293700Z"
    }
   },
   "id": "b489ecc8532c36b0",
   "execution_count": 244
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char Tokenization: ['T', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g', ' ', 't', 'h', 'a', 't', ' ']\n",
      "\n",
      "Word Tokenization: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'that']\n",
      "\n",
      "Stemming : ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', 'that']\n",
      "\n",
      "Lemmatization : ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', 'that']\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog that \"\n",
    "\n",
    "characters = character_tokenization(text)\n",
    "words = word_tokenization(text)\n",
    "stems = stem_tokenization(text) \n",
    "print(f\"Char Tokenization: {characters[:51]}\", end='\\n\\n')\n",
    "print(f\"Word Tokenization: {words}\", end='\\n\\n')\n",
    "print(f\"Stemming : {stems}\", end='\\n\\n')\n",
    "print(f\"Lemmatization : {stems}\", end='\\n\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T02:17:59.457705500Z",
     "start_time": "2025-03-25T02:17:59.445557Z"
    }
   },
   "id": "a5acb9202aca0d2e",
   "execution_count": 259
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#4. Dynamic Lexicon\n",
    "\n",
    "stop = ['.', '!', '?', ':', ';', 'it', 'he', 'had', 'i', 'if', 'a', 'an', 'as', 'to', 'the', 'was', \\\n",
    "        'of', 'these', 'in', 'on', 'at', 'with', 'is', 'am', 'are','we', 'be', 'been', 'im', 'so', 'that',\\\n",
    "        'to', 'was', ]\n",
    "def generate_lexicon(text: str, window_size: int = 3):\n",
    "  '''\n",
    "  Modify our original lexicon and append new words\n",
    "  Filters out predefined stopwords and punctuation markers\n",
    "  Assigns new scores using a sliding window + (pos_words - neg_words) \n",
    "  :param text: block of text to be lexicicalized \n",
    "  :return: something\n",
    "  '''\n",
    "  static_lexicon = lexicon.copy() \n",
    "  #some predefined stop words\n",
    "\n",
    "  #alternatively, stop_words = set(stopwords.words('english'))\n",
    "  \n",
    "  #we need lists of positive + negative words in statically defined lexicon\n",
    "  pos_words = [word[0] for word in static_lexicon.items() if int(word[1]) > 0]\n",
    "  neg_words = [word[0] for word in static_lexicon.items() if  int(word[1]) < 0]\n",
    " \n",
    "  #no lets iterate through our tokenized text \n",
    "  dynamic_lexicon = {} \n",
    "  tokens =  stem_tokenization(text)\n",
    "  \n",
    "  for index, token in enumerate(tokens):\n",
    "    #print(f\"EVALUATING SCORE FOR <{token}>\")\n",
    "    #if its in our predefined list of stop words (which includes puncutation) or if its like a number or special character, just skip it\n",
    "    if token in stop or not token.isalpha(): \n",
    "      #print(f\"Token <{token}> is a stop word\")\n",
    "      continue\n",
    "    #if we already have this token accounted for, skip it \n",
    "    if token in static_lexicon or token in amplifier_words:\n",
    "      #print(f\"Token <{token}> is already defined\")\n",
    "      continue\n",
    "    positives = 0\n",
    "    negatives = 0    \n",
    "    #establish the bounds of our window\n",
    "    start = max(0, index - window_size) #if curr_index - window size is negative, we will get out of bounds. So start at 0 if thats the case\n",
    "    end   = min(len(tokens), index + window_size + 1) #if the window size escapes the size of our tokens, just end at the final token\n",
    "\n",
    "    #print(f\"start index => <{start}\\tend index => {end}\")\n",
    "    for j in range(start, end):\n",
    "      #print(f\"\\twindow token at j <{j} => {tokens[j]}\", end = ' ')\n",
    "      #skip the word we are evaluating at the moment\n",
    "      if j == index or tokens[j] in stop:\n",
    "        #print(\"CONTINUING\")\n",
    "        continue\n",
    "\n",
    "      candidate = tokens[j]\n",
    "\n",
    "      #if tokens[j] in static_lexicon:\n",
    "        #print(f\"\\t{static_lexicon[tokens[j]]}\", end=' ')\n",
    "      #print(\"\")\n",
    "      if candidate in pos_words:\n",
    "        positives+=1\n",
    "      elif candidate in neg_words:\n",
    "        negatives+=1\n",
    "      \n",
    "      score = positives - negatives\n",
    "      dynamic_lexicon[token] = score\n",
    "    #print(f\"Final entry for token <{token}> => {dynamic_lexicon[token]}\", end='\\n\\n')\n",
    "    \n",
    "  return dynamic_lexicon"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T09:31:23.847093800Z",
     "start_time": "2025-03-24T09:31:23.838428800Z"
    }
   },
   "id": "54ebbb4803134722",
   "execution_count": 246
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wa': 0, 'veri': 1, 'hear': 1, 'about': -1, 'your': 0, 'wonder': 0, 'sorri': -1, 'hassl': -1, 'glad': 0, 'fantast': 0, 'superwoman': 0}\n"
     ]
    }
   ],
   "source": [
    "text = \"I was very good to hear about your wonderful vacation! Im so sorry about that stupid hassle at the airport, glad it was fantastic Superwoman !\"\n",
    "print(generate_lexicon(text, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T00:11:08.512480400Z",
     "start_time": "2025-03-24T00:11:08.507049700Z"
    }
   },
   "id": "88eed1b7030ac46a",
   "execution_count": 196
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### *Notice how this behavior isn't exactly what we we're shooting for.*\n",
    "1. <'wonderful'> was converted to <'wonder'> via tokenization, went unrecognized as a positive seed\n",
    "2. <'very'> was converted to <'veri'> via tokenization, went unrecognized as a amplifier word\n",
    "3. 'superwoman' preceded by fantastic, yet neutral score was calculated\n",
    "4. I guess my statically defined lexicon does not define 'glad' as a positive word\n",
    "\n",
    "This is clearly problematic. To fix this, I hypothesised that tokenizing my lexicons (positive, negative, stop, amplifiers...) will yield better, more expected results.\n",
    "Logically, what is applied to one should be applied to all, for consistency. Words with similar stems "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efa55871e5fb66c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T00:03:15.154980500Z",
     "start_time": "2025-03-24T00:03:15.149871200Z"
    }
   },
   "id": "5fe72cb735d9357a",
   "execution_count": 190
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c7494706682dd8ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
